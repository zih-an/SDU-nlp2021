{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbe0ef9",
   "metadata": {},
   "source": [
    "# Lab3: Clustering & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e2adc2",
   "metadata": {},
   "source": [
    "## Data\n",
    "`Twitter_data`: This file contains 29846 data,and each of them has 8 items\n",
    "- \"userName\":用户名\n",
    "- \"clusterNo\":类别\n",
    "- \"text\":Twitter内容\n",
    "- \"timeStr\":时间戳\n",
    "- \"tweeId\":用户Id\n",
    "- \"errorCode\":状态码\n",
    "- \"textCleaned\":去除链接等特殊符号只保留文本的处理\n",
    "- \"relevance\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b824f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 将数据读取成dict格式便于后续的操作\n",
    "Twitter_data=[]\n",
    "with open(\"Twitter_data\")as f:\n",
    "    for line in f:\n",
    "        # print(line)\n",
    "        Twitter_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9edfb4",
   "metadata": {},
   "source": [
    "##  Tokenize\n",
    "直接按空格分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bec5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_textCleaned = [] \n",
    "words = set([]) \n",
    "for item in Twitter_data: \n",
    "    tokens = item[\"textCleaned\"].split(\" \")\n",
    "    token_textCleaned.append(tokens) \n",
    "    for token in tokens:\n",
    "        words.add(token) \n",
    "\n",
    "num_words_max = len(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8838ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12227\n"
     ]
    }
   ],
   "source": [
    "print(num_words_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b64be4",
   "metadata": {},
   "source": [
    "## Vectoring\n",
    "Bow(Bag of Words) 词袋模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab08c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "bow_dict = dict()\n",
    "for i, word in enumerate(words):\n",
    "    bow_dict[word] = i\n",
    "\n",
    "vec_textCleaned = np.zeros((len(token_textCleaned), num_words_max)) \n",
    "for i, sentence in enumerate(token_textCleaned):\n",
    "    for word in sentence:\n",
    "        j = bow_dict[word] \n",
    "        vec_textCleaned[i][j] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6c8a6",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "聚类文本时因为数据太大，会比较慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4d06845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    def __init__(self, data, num_classes, max_iter=200):\n",
    "        self.num_classes = num_classes \n",
    "        self.src_data = data\n",
    "        self.max_iter = max_iter\n",
    "        self.m_examples, self.n_features = data.shape\n",
    "        \n",
    "        self.label = np.zeros(self.m_examples)\n",
    "        self.clusters = [[] for i in range(num_classes)]  ## idx_list of each class in src_data \n",
    "        ## center vectors\n",
    "        init_cen_idx = np.random.choice(self.m_examples, num_classes, replace=False)  ## init randomly first\n",
    "        self.centroid = self.src_data[init_cen_idx]\n",
    "    \n",
    "    def run(self, threshold=1e-2):\n",
    "        for _ in range(self.max_iter):\n",
    "            print(\"cluster\")\n",
    "            self.clusters = [[] for i in range(self.num_classes)]\n",
    "            self._cluster(self.centroid) \n",
    "            print(\"centroid\")\n",
    "            newCentroid = self._genCentroid(self.clusters)\n",
    "            if self._edis(self.centroid, newCentroid) < threshold: \n",
    "                print(\"bbbbbreak\")\n",
    "                break \n",
    "            self.centroid = newCentroid \n",
    "        return self.label \n",
    "        \n",
    "    def _cluster(self, centroid):\n",
    "        for idx, sample in enumerate(self.src_data):\n",
    "            lbl, dis = -1, float(\"inf\")\n",
    "            for cls in range(self.num_classes):\n",
    "                tmp = np.sum((sample - centroid[cls])**2)\n",
    "                if tmp < dis:\n",
    "                    lbl = cls\n",
    "                    dis = tmp\n",
    "            self.label[idx] = lbl  # record the class for this sample\n",
    "            self.clusters[lbl].append(idx)  # add this sample to the class \n",
    "            \n",
    "    def _genCentroid(self, clusters):\n",
    "        newCentroid = np.zeros((self.num_classes, self.n_features)) \n",
    "        for i, cluster in enumerate(clusters):\n",
    "            cluster_mean = np.mean(self.src_data[cluster], axis=0)\n",
    "            newCentroid[i] = cluster_mean\n",
    "        return newCentroid \n",
    "    \n",
    "    def _edis(self, cen1, cen2):\n",
    "        return np.sum(np.sqrt(np.sum((cen1-cen2)**2, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c6d1bc85",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "centroid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\numpy\\core\\_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "centroid\n",
      "cluster\n",
      "centroid\n"
     ]
    }
   ],
   "source": [
    "# km = KMeans(np.array([[0, 1, 1], [1, 0, 0], [1, 1, 0], [1, 0, 1], [0, 0, 1], [1, 1, 1]]), 3, 10) \n",
    "# lbl = km.run() \n",
    "# print(lbl)\n",
    "km = KMeans(vec_textCleaned, 200, 3) \n",
    "lbl = km.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3b4ed133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.01801802 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "[ 20.  20.  20. ...  20. 113. 166.]\n"
     ]
    }
   ],
   "source": [
    "print(km.centroid)\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc824c",
   "metadata": {},
   "source": [
    "## Test\n",
    "思想：用kmeans聚类后的结果，与原本的分类情况比较。统计聚类之后，每一个类别中，原分类的情况。\n",
    "1. 统计原分类，最多出现的分类的个数，作为这个聚类的类别\n",
    "2. 计算比例：聚类类别中，属于这个类别的项 / 整个聚类类别的所有元素个数。将此作为正确比例\n",
    "3. 求每一组聚类类别正确比例的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bc229183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n",
      "344\n"
     ]
    }
   ],
   "source": [
    "print(Twitter_data[23522][\"clusterNo\"])\n",
    "print(Twitter_data[23509][\"clusterNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aec81762",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluation(data_dict, clusters):\n",
    "    evl = np.zeros(len(clusters)) \n",
    "    for idx, clstr in enumerate(clusters):\n",
    "        clsOrg = [Twitter_data[i][\"clusterNo\"] for i in clstr] \n",
    "        if len(clsOrg) > 0: \n",
    "            mainCls = max(clsOrg, key=clsOrg.count)  # 统计出现最多次的元素\n",
    "            evl[idx] = clsOrg.count(mainCls) / len(clsOrg)\n",
    "        else:\n",
    "            evl[idx] = 0\n",
    "    return evl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "46c23944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9471817805273113\n"
     ]
    }
   ],
   "source": [
    "evl = evaluation(Twitter_data, km.clusters)\n",
    "print(np.mean(evl))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tensorflowenv",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
