{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d851d86",
   "metadata": {},
   "source": [
    "# Lab1 Sentiment Analysis using Naive Bayes\n",
    "## Baseline Algorithm\n",
    "1. Tokenization (分词)\n",
    "2. Feature Extraction (tfidf)\n",
    "3. Naive Bayes\n",
    "\n",
    "### Data\n",
    "- 文档形式，模型：$P(W) = P(w_1, w_2, ..., w_n)$\n",
    "- 情感特征(y)，polarity：+/- (1/0)\n",
    "    + 判断：$P(y|W), y=0,1$取最大值作为类别\n",
    "- Unigram, Bigram, Trigram, ...N-gram\n",
    "    + 普通的贝叶斯分类器+laplace smoothing是unigram\n",
    "    + 对于文本，N-gram效果应该会更好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf981a7",
   "metadata": {},
   "source": [
    "## File Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392f3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "def loadData(flagTrain = True):\n",
    "    path = './aclImdb/'\n",
    "    if flagTrain:\n",
    "        path += \"train/\"\n",
    "    else:\n",
    "        path += \"test/\"\n",
    "    \n",
    "    pos_path = path + 'pos/'\n",
    "    neg_path = path + 'neg/'\n",
    "    pos_files = [pos_path + x for x in \n",
    "                 filter(lambda x: x.endswith('.txt'), os.listdir(pos_path))]\n",
    "    neg_files = [neg_path + x for x in \n",
    "                 filter(lambda x: x.endswith('.txt'), os.listdir(neg_path))]\n",
    "    pos_list = [open(x, 'r', encoding='utf-8').read().lower() for x in pos_files]\n",
    "    neg_list = [open(x, 'r', encoding='utf-8').read().lower() for x in neg_files]\n",
    "    data_list = pos_list + neg_list\n",
    "    label_list = [1] * len(pos_list) + [0] * len(neg_list)\n",
    "    \n",
    "    # shuffle if you'd like ===========================\n",
    "    if flagTrain:\n",
    "        merged_data = list(zip(data_list, label_list))\n",
    "        shuffle(merged_data)\n",
    "        data_list, label_list = list(zip(*merged_data))\n",
    "    return list(data_list), list(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9206bdbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 25000\n",
      "<class 'list'> 25000\n"
     ]
    }
   ],
   "source": [
    "data_list, label_list = loadData()\n",
    "print(type(data_list), len(data_list))\n",
    "print(type(label_list), len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d5c594",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efa1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_vocab_size = 50000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(data_list)\n",
    "tf_idf_data = tokenizer.texts_to_matrix(data_list, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb89152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 0]\n",
      "data shape:  (25000, 50000)\n",
      "label shape:  (25000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "label_list = np.array(label_list)\n",
    "\n",
    "print(label_list)\n",
    "print(\"data shape: \", tf_idf_data.shape)\n",
    "print(\"label shape: \", label_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c709b6",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "本例中，每个输入数据为文本，文本通过tfidf预处理后（见appendix），相当于每个值是权重，替换原来公式中的count的位置。\n",
    "1. 每个词算一个特征，即$x_j^{(i)}$算一个词，而它的取值是0或1，即有或没有\n",
    "2. 所以此时在count的时候，就是0或1两种情况。count的过程，就是相加的过程。\n",
    "3. 经过tfidf处理之后，每个词$x_j^{(i)}$变成了一个浮点数，可以看作权重\n",
    "4. 每个count的位置，变成相加求和tfidf的值\n",
    "\n",
    "> 相当于，\n",
    "> - count(y)现在是把y类型的，所有tfidf数值加起来\n",
    "> - count(x, y)现在是把y类型的，所有x特征的tfidf数值加起来\n",
    "\n",
    "在最终预测的时候，不需要用tfidf，只需要做词语有/无的向量，使用训练好的$p(y)$和$p(X|y)$计算即可。\n",
    "> 使用log处理，加法比乘法更快：\n",
    "> $$p(y|X) ≈ p(y)p(X|y) => log(p(y)) + log(p(X|y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ac55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def fit(self, X, y):\n",
    "        self.num_classes = 2  # neg/pos = 0/1\n",
    "        self.m_examples = y.shape[0] \n",
    "        ## p(X|y)\n",
    "        self.prob_Xy_arr = np.zeros((self.num_classes, X.shape[1]), dtype=np.float64)\n",
    "        count_y = np.zeros((self.num_classes, 1))\n",
    "        for i in range(self.m_examples):\n",
    "            ith_lbl = y[i] \n",
    "            self.prob_Xy_arr[ith_lbl] += X[i] \n",
    "            count_y[ith_lbl] += np.sum(X[i])\n",
    "        self.prob_Xy_arr =  (self.prob_Xy_arr + 1) / (count_y + X.shape[1])\n",
    "        \n",
    "        ## p(y)\n",
    "        self.prob_y_arr = np.zeros(self.num_classes, dtype=np.float64)\n",
    "        for i in range(self.num_classes): \n",
    "            self.prob_y_arr[i] = sum(y==i) / self.m_examples \n",
    "    \n",
    "    def predict(self, X):\n",
    "        m_test = X.shape[0] \n",
    "        labels = np.zeros(m_test)\n",
    "        for i in range(m_test): \n",
    "            y, prob = None, float('-inf') \n",
    "            for lbl in range(self.num_classes):\n",
    "                sc = np.sum(X[i] * np.log(self.prob_Xy_arr[lbl]) + np.log(self.prob_y_arr[lbl]))\n",
    "                if sc > prob:\n",
    "                    prob = sc \n",
    "                    y = lbl \n",
    "            labels[i] = y\n",
    "        return labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6372d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(tf_idf_data, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4352c409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.51975928e-07 1.93160878e-02 4.32211781e-03 ... 1.51975928e-07\n",
      "  1.52403570e-06 2.57939809e-06]\n",
      " [1.44923561e-07 1.86923348e-02 4.14379472e-03 ... 2.76170361e-06\n",
      "  1.45331359e-06 1.44923561e-07]]\n",
      "[0.5 0.5]\n",
      "0.9999999999999988\n"
     ]
    }
   ],
   "source": [
    "print(nb.prob_Xy_arr)\n",
    "print(nb.prob_y_arr)\n",
    "\n",
    "print(np.sum(nb.prob_Xy_arr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed6aeb",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8791500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 25000\n",
      "<class 'list'> 25000\n"
     ]
    }
   ],
   "source": [
    "testdata, testlabel = loadData(False) \n",
    "print(type(testdata), len(testdata))\n",
    "print(type(testlabel), len(testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b12968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test label shape:  (25000,)\n",
      "[1 1 1 ... 0 0 0]\n",
      "test data shape:  (25000, 50000)\n",
      "[[ 0.          0.          2.05416196 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         21.25195642  2.88365037 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         21.25195642  2.85265447 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.         26.4249195   2.8202164  ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         24.16521815  2.05416196 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         33.44419303  2.63059898 ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "testdata_tfidf = tokenizer.texts_to_matrix(testdata, mode='tfidf') \n",
    "testlabel = np.array(testlabel)\n",
    "print(\"test label shape: \", testlabel.shape)\n",
    "print(testlabel)\n",
    "print(\"test data shape: \", testdata_tfidf.shape)\n",
    "print(testdata_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b115fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 0. 1.]\n",
      "accuracy:  0.785\n"
     ]
    }
   ],
   "source": [
    "predlabels = nb.predict(testdata_tfidf)\n",
    "print(predlabels)\n",
    "acc = predlabels==testlabel\n",
    "print(\"accuracy: \", np.sum(acc) / testlabel.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139969c0",
   "metadata": {},
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8833d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9568b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sklearn] TfidfVectorizer accuracy:   0.830\n"
     ]
    }
   ],
   "source": [
    "## 使用sklearn的预处理 + bayes\n",
    "tf_vectorizer = TfidfVectorizer() # CountVectorizer=0.814；TfidfVectorizer=0.830\n",
    "X_train_tf = tf_vectorizer.fit_transform(data_list)\n",
    "X_test_tf = tf_vectorizer.transform(testdata)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, label_list)\n",
    "y_pred = naive_bayes_classifier.predict(X_test_tf)\n",
    "score1 = metrics.accuracy_score(testlabel, y_pred)\n",
    "print(\"[sklearn] TfidfVectorizer accuracy:   %0.3f\" % score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31787e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensorflow] accuracy:   0.785\n"
     ]
    }
   ],
   "source": [
    "## 使用上述内容相同的数据(tensorflow 预处理) + sklearn的bayes = 0.785\n",
    "naive_bayes_classifier.fit(tf_idf_data, label_list)\n",
    "y_pred = naive_bayes_classifier.predict(testdata_tfidf)\n",
    "score2 = metrics.accuracy_score(testlabel, y_pred)\n",
    "print(\"[tensorflow] accuracy:   %0.3f\" % score2)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tensorflowenv",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
